---
title: "trawl data reduction"
output: html_notebook
---
##Trawl Data

The main goal is to reduce the original trawl dataset to as little data as possible, only keeping what will possibly be used in analyses. 

```{r setup}
library(tidyr) #for reshaping data
library(dplyr) #for data manipulation [helpful cheatsheet](https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)
library(lubridate) #helps us work with dates [helpful cheatsheet](https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf)
library(ggplot2) #helps us with visualization [helpful cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf)
library(data.table) #deals with large datasets
```

Here, we'll upload the raw data from [Ocean Adapt](https://github.com/pinskylab/OceanAdapt), accessed Fall 2019. 

Raw files stored in [Box folder](https://rutgers.app.box.com/folder/90175159431)
```{r data upload}
#NEUS
load("neus_Survdat.Rdata")
load("neus_SVSPP.Rdata")

#Gulf
load("gulf_trawl_data.Rdata")

#SEUS

load("seus_trawl_data.Rdata")

#Scotian Shelf

shelf_file_list <- Sys.glob(path="RV*.RData") #this creates a list with the name of all the Scotian Shelf Files, which all conveniently start with RV and end with .Rdata. the * says "with anything in between"

for (i in 1:length(shelf_file_list)) { #this for loop goes through every character string in the list and loads them into your environment from the repository folder
  load(shelf_file_list[i])
}

```


###NEUS Data

Merge NEUS datasets so that scientific and common name are displayed on the main dataset.
```{r merge NEUS}
neus_full <- left_join(spp, survdat, by = "SVSPP")
```

Reduce NEUS data.
```{r reduce NEUS}
neus_reduced <- neus_full[, c("TOW", "CATCHSEX", "YEAR", "SEASON", "LAT", "LON", "EST_TOWDATE", "DEPTH", "SURFTEMP", "BOTTEMP", "ABUNDANCE", "BIOMASS", "LENGTH", "NUMLEN", "SCINAME", "COMNAME")]
```

Save NEUS data.
```{r save NEUS}
save(neus_reduced, file = "neus_reduced.Rdata")
```


###GULF Data

Reduce GULF data.
```{r reduce GULF} 
#this must be done before merging the data to avoid memory exhuastion
glf_individ_red <- glf_individ[, c("CRUISE_NO", "P_STA_NO", "GENUS_GLF", "SPEC_GLF", "MEASCD_GLF", "LEN_GLF", "SEX_GLF", "MAT_GLF")]
glf_trawl_red <- glf_trawl_specifics[, c("CRUISE_NO", "P_STA_NO", "MO_DAY_YR", "TEMP_SSURF", "TEMP_BOT", "TOW_NO", "DECSLAT", "DECSLON", "DECELAT", "DECELON")]
```

Merge GULF data.
```{r merge GULF}
#first look at column names to see which ones overlap, and therefore which we can use to connect the two datasets
colnames(glf_individ_red)
colnames(glf_trawl_red)

glf_reduced <- left_join(glf_individ_red, glf_trawl_red, by = c("CRUISE_NO", "P_STA_NO")) #vector memory exhausted
#this appears to be a computer problem, as it's running on my computer

#using Cruise_No and P_Sta_No make sense because the same station would never be visited more than once during a cruise

```


_________________
####Final Goal: Merge all datasets so that we can look at how length varies across latitude, temperature, and other predictor variables. Before, we'll have to verify that all units are the same! It looks like because we'll have to continue doing merges, we'll have to do it on a lab computer. 

Ex: 

| year | species | location | temperature | length | number |
|---|---|---|---|---|---|
|1970 | Homarus americanus | 34.456 | 3 | 45 | number |